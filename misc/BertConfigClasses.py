class BertModelConfigs(object):

    def __init__(self):

        #https://huggingface.co/bert-base-cased/blob/main/config.json for bert-base-cased configuration.
        self.bert_base_cased_gating_all_3_layers_512sl = {"vocab_size":28996, "hidden_size":768,"num_hidden_layers":12,
                                                    "num_attention_heads":12,"intermediate_size":3072,
                                                    "hidden_act":"gelu","hidden_dropout_prob":0.1,
                                                    "attention_probs_dropout_prob":0.1, "type_vocab_size":2,
                                                    "initializer_range":0.02,"layer_norm_eps":1e-12,
                                                    "pad_token_id":0,"position_embedding_type":"absolute",
                                                    "use_cache":True,"classifier_dropout":None,
                                                    "cls_dense_layer_number_of_options":1,"gating_block_num_layers":3,
                                                    "gating_block_end":True, "gating_block_end_position":9,
                                                    "gating_block_middle":True, "gating_block_middle_position":6,
                                                    "gating_block_start":True, "gating_block_start_position":3,
                                                    "nm_gating":True,"is_diagnostics":False,
                                                    "max_position_embeddings":512, "max_seq_len":512, "model_type":"bert"}
        self.bert_base_cased_gating_end_only_3_layers_512sl = {"vocab_size":28996, "hidden_size":768,"num_hidden_layers":12,
                                                         "num_attention_heads":12,"intermediate_size":3072,
                                                         "hidden_act":"gelu","hidden_dropout_prob":0.1,
                                                         "attention_probs_dropout_prob":0.1, "type_vocab_size":2,
                                                         "initializer_range":0.02,"layer_norm_eps":1e-12,
                                                         "pad_token_id":0,"position_embedding_type":"absolute",
                                                         "use_cache":True,"classifier_dropout":None,
                                                         "cls_dense_layer_number_of_options":1,"gating_block_num_layers":3,
                                                         "gating_block_end":True, "gating_block_end_position":9,
                                                         "gating_block_middle":False, "gating_block_middle_position":6,
                                                         "gating_block_start":False, "gating_block_start_position":3,
                                                         "nm_gating":True,"is_diagnostics":False,
                                                         "max_position_embeddings":512, "max_seq_len":512, "model_type":"bert"}
        self.bert_base_cased_gating_middle_only_3_layers_512sl = {"vocab_size":28996, "hidden_size":768,"num_hidden_layers":12,
                                                            "num_attention_heads":12,"intermediate_size":3072,
                                                            "hidden_act":"gelu","hidden_dropout_prob":0.1,
                                                            "attention_probs_dropout_prob":0.1, "type_vocab_size":2,
                                                            "initializer_range":0.02,"layer_norm_eps":1e-12,
                                                            "pad_token_id":0,"position_embedding_type":"absolute",
                                                            "use_cache":True,"classifier_dropout":None,
                                                            "cls_dense_layer_number_of_options":1,"gating_block_num_layers":3,
                                                            "gating_block_end":False, "gating_block_end_position":9,
                                                            "gating_block_middle":True, "gating_block_middle_position":6,
                                                            "gating_block_start":False, "gating_block_start_position":3,
                                                            "nm_gating":True,"is_diagnostics":False,
                                                            "max_position_embeddings":512, "max_seq_len":512, "model_type":"bert"}
        self.bert_base_cased_gating_start_only_3_layers_512sl = {"vocab_size":28996, "hidden_size":768,"num_hidden_layers":12,
                                                           "num_attention_heads":12,"intermediate_size":3072,
                                                           "hidden_act":"gelu","hidden_dropout_prob":0.1,
                                                           "attention_probs_dropout_prob":0.1, "type_vocab_size":2,
                                                           "initializer_range":0.02,"layer_norm_eps":1e-12,
                                                           "pad_token_id":0,"position_embedding_type":"absolute",
                                                           "use_cache":True,"classifier_dropout":None,
                                                           "cls_dense_layer_number_of_options":1,"gating_block_num_layers":3,
                                                           "gating_block_end":False, "gating_block_end_position":9,
                                                           "gating_block_middle":False, "gating_block_middle_position":6,
                                                           "gating_block_start":True, "gating_block_start_position":3,
                                                           "nm_gating":True,"is_diagnostics":False,
                                                           "max_position_embeddings":512, "max_seq_len":512, "model_type":"bert"}


        # https://huggingface.co/bert-base-cased/blob/main/config.json for bert-base-cased configuration.
        self.bert_base_cased_gating_all_1_layers_512sl = {"vocab_size": 28996, "hidden_size": 768, "num_hidden_layers": 12,
                                                    "num_attention_heads": 12, "intermediate_size": 3072,
                                                    "hidden_act": "gelu", "hidden_dropout_prob": 0.1,
                                                    "attention_probs_dropout_prob": 0.1, "type_vocab_size": 2,
                                                    "initializer_range": 0.02, "layer_norm_eps": 1e-12,
                                                    "pad_token_id": 0, "position_embedding_type": "absolute",
                                                    "use_cache": True, "classifier_dropout": None,
                                                    "cls_dense_layer_number_of_options": 1,
                                                    "gating_block_num_layers": 1,
                                                    "gating_block_end": True, "gating_block_end_position": 9,
                                                    "gating_block_middle": True, "gating_block_middle_position": 6,
                                                    "gating_block_start": True, "gating_block_start_position": 3,
                                                    "nm_gating": True, "is_diagnostics": False,
                                                    "max_position_embeddings": 512, "max_seq_len": 512, "model_type":"bert"}
        self.bert_base_cased_gating_end_only_1_layers_512sl = {"vocab_size": 28996, "hidden_size": 768,
                                                         "num_hidden_layers": 12,
                                                         "num_attention_heads": 12, "intermediate_size": 3072,
                                                         "hidden_act": "gelu", "hidden_dropout_prob": 0.1,
                                                         "attention_probs_dropout_prob": 0.1, "type_vocab_size": 2,
                                                         "initializer_range": 0.02, "layer_norm_eps": 1e-12,
                                                         "pad_token_id": 0, "position_embedding_type": "absolute",
                                                         "use_cache": True, "classifier_dropout": None,
                                                         "cls_dense_layer_number_of_options": 1,
                                                         "gating_block_num_layers": 1,
                                                         "gating_block_end": True, "gating_block_end_position": 9,
                                                         "gating_block_middle": False,
                                                         "gating_block_middle_position": 6,
                                                         "gating_block_start": False, "gating_block_start_position": 3,
                                                         "nm_gating": True, "is_diagnostics": False,
                                                         "max_position_embeddings": 512, "max_seq_len": 512, "model_type":"bert"}
        self.bert_base_cased_gating_middle_only_1_layers_512sl = {"vocab_size": 28996, "hidden_size": 768,
                                                            "num_hidden_layers": 12,
                                                            "num_attention_heads": 12, "intermediate_size": 3072,
                                                            "hidden_act": "gelu", "hidden_dropout_prob": 0.1,
                                                            "attention_probs_dropout_prob": 0.1, "type_vocab_size": 2,
                                                            "initializer_range": 0.02, "layer_norm_eps": 1e-12,
                                                            "pad_token_id": 0, "position_embedding_type": "absolute",
                                                            "use_cache": True, "classifier_dropout": None,
                                                            "cls_dense_layer_number_of_options": 1,
                                                            "gating_block_num_layers": 1,
                                                            "gating_block_end": False, "gating_block_end_position": 9,
                                                            "gating_block_middle": True,
                                                            "gating_block_middle_position": 6,
                                                            "gating_block_start": False,
                                                            "gating_block_start_position": 3,
                                                            "nm_gating": True, "is_diagnostics": False,
                                                            "max_position_embeddings": 512, "max_seq_len": 512, "model_type":"bert"}
        self.bert_base_cased_gating_start_only_1_layers_512sl = {"vocab_size": 28996, "hidden_size": 768,
                                                           "num_hidden_layers": 12,
                                                           "num_attention_heads": 12, "intermediate_size": 3072,
                                                           "hidden_act": "gelu", "hidden_dropout_prob": 0.1,
                                                           "attention_probs_dropout_prob": 0.1, "type_vocab_size": 2,
                                                           "initializer_range": 0.02, "layer_norm_eps": 1e-12,
                                                           "pad_token_id": 0, "position_embedding_type": "absolute",
                                                           "use_cache": True, "classifier_dropout": None,
                                                           "cls_dense_layer_number_of_options": 1,
                                                           "gating_block_num_layers": 1,
                                                           "gating_block_end": False, "gating_block_end_position": 9,
                                                           "gating_block_middle": False,
                                                           "gating_block_middle_position": 6,
                                                           "gating_block_start": True, "gating_block_start_position": 3,
                                                           "nm_gating": True, "is_diagnostics": False,
                                                           "max_position_embeddings": 512, "max_seq_len": 512, "model_type":"bert"}


        # https://huggingface.co/bert-base-cased/blob/main/config.json for bert-base-cased configuration.
        self.bert_base_cased_no_gating_all_3_layers_512sl = {"vocab_size": 28996, "hidden_size": 768, "num_hidden_layers": 12,
                                                    "num_attention_heads": 12, "intermediate_size": 3072,
                                                    "hidden_act": "gelu", "hidden_dropout_prob": 0.1,
                                                    "attention_probs_dropout_prob": 0.1, "type_vocab_size": 2,
                                                    "initializer_range": 0.02, "layer_norm_eps": 1e-12,
                                                    "pad_token_id": 0, "position_embedding_type": "absolute",
                                                    "use_cache": True, "classifier_dropout": None,
                                                    "cls_dense_layer_number_of_options": 1,
                                                    "gating_block_num_layers": 3,
                                                    "gating_block_end": True, "gating_block_end_position": 9,
                                                    "gating_block_middle": True, "gating_block_middle_position": 6,
                                                    "gating_block_start": True, "gating_block_start_position": 3,
                                                    "nm_gating": False, "is_diagnostics": False,
                                                    "max_position_embeddings": 512, "max_seq_len": 512, "model_type":"bert"}
        self.bert_base_cased_no_gating_end_only_3_layers_512sl = {"vocab_size": 28996, "hidden_size": 768,
                                                         "num_hidden_layers": 12,
                                                         "num_attention_heads": 12, "intermediate_size": 3072,
                                                         "hidden_act": "gelu", "hidden_dropout_prob": 0.1,
                                                         "attention_probs_dropout_prob": 0.1, "type_vocab_size": 2,
                                                         "initializer_range": 0.02, "layer_norm_eps": 1e-12,
                                                         "pad_token_id": 0, "position_embedding_type": "absolute",
                                                         "use_cache": True, "classifier_dropout": None,
                                                         "cls_dense_layer_number_of_options": 1,
                                                         "gating_block_num_layers": 3,
                                                         "gating_block_end": True, "gating_block_end_position": 9,
                                                         "gating_block_middle": False,
                                                         "gating_block_middle_position": 6,
                                                         "gating_block_start": False, "gating_block_start_position": 3,
                                                         "nm_gating": False, "is_diagnostics": False,
                                                         "max_position_embeddings": 512, "max_seq_len": 512, "model_type":"bert"}
        self.bert_base_cased_no_gating_middle_only_3_layers_512sl = {"vocab_size": 28996, "hidden_size": 768,
                                                            "num_hidden_layers": 12,
                                                            "num_attention_heads": 12, "intermediate_size": 3072,
                                                            "hidden_act": "gelu", "hidden_dropout_prob": 0.1,
                                                            "attention_probs_dropout_prob": 0.1, "type_vocab_size": 2,
                                                            "initializer_range": 0.02, "layer_norm_eps": 1e-12,
                                                            "pad_token_id": 0, "position_embedding_type": "absolute",
                                                            "use_cache": True, "classifier_dropout": None,
                                                            "cls_dense_layer_number_of_options": 1,
                                                            "gating_block_num_layers": 3,
                                                            "gating_block_end": False, "gating_block_end_position": 9,
                                                            "gating_block_middle": True,
                                                            "gating_block_middle_position": 6,
                                                            "gating_block_start": False,
                                                            "gating_block_start_position": 3,
                                                            "nm_gating": False, "is_diagnostics": False,
                                                            "max_position_embeddings": 512, "max_seq_len": 512, "model_type":"bert"}
        self.bert_base_cased_no_gating_start_only_3_layers_512sl = {"vocab_size": 28996, "hidden_size": 768,
                                                           "num_hidden_layers": 12,
                                                           "num_attention_heads": 12, "intermediate_size": 3072,
                                                           "hidden_act": "gelu", "hidden_dropout_prob": 0.1,
                                                           "attention_probs_dropout_prob": 0.1, "type_vocab_size": 2,
                                                           "initializer_range": 0.02, "layer_norm_eps": 1e-12,
                                                           "pad_token_id": 0, "position_embedding_type": "absolute",
                                                           "use_cache": True, "classifier_dropout": None,
                                                           "cls_dense_layer_number_of_options": 1,
                                                           "gating_block_num_layers": 3,
                                                           "gating_block_end": False, "gating_block_end_position": 9,
                                                           "gating_block_middle": False,
                                                           "gating_block_middle_position": 6,
                                                           "gating_block_start": True, "gating_block_start_position": 3,
                                                           "nm_gating": False, "is_diagnostics": False,
                                                           "max_position_embeddings": 512, "max_seq_len": 512, "model_type":"bert"}

        # https://huggingface.co/bert-base-cased/blob/main/config.json for bert-base-cased configuration.
        self.bert_base_cased_no_gating_all_1_layers_512sl = {"vocab_size": 28996, "hidden_size": 768, "num_hidden_layers": 12,
                                                    "num_attention_heads": 12, "intermediate_size": 3072,
                                                    "hidden_act": "gelu", "hidden_dropout_prob": 0.1,
                                                    "attention_probs_dropout_prob": 0.1, "type_vocab_size": 2,
                                                    "initializer_range": 0.02, "layer_norm_eps": 1e-12,
                                                    "pad_token_id": 0, "position_embedding_type": "absolute",
                                                    "use_cache": True, "classifier_dropout": None,
                                                    "cls_dense_layer_number_of_options": 1,
                                                    "gating_block_num_layers": 1,
                                                    "gating_block_end": True, "gating_block_end_position": 9,
                                                    "gating_block_middle": True, "gating_block_middle_position": 6,
                                                    "gating_block_start": True, "gating_block_start_position": 3,
                                                    "nm_gating": False, "is_diagnostics": False,
                                                    "max_position_embeddings": 512, "max_seq_len": 512, "model_type":"bert"}
        self.bert_base_cased_no_gating_end_only_1_layers_512sl = {"vocab_size": 28996, "hidden_size": 768,
                                                         "num_hidden_layers": 12,
                                                         "num_attention_heads": 12, "intermediate_size": 3072,
                                                         "hidden_act": "gelu", "hidden_dropout_prob": 0.1,
                                                         "attention_probs_dropout_prob": 0.1, "type_vocab_size": 2,
                                                         "initializer_range": 0.02, "layer_norm_eps": 1e-12,
                                                         "pad_token_id": 0, "position_embedding_type": "absolute",
                                                         "use_cache": True, "classifier_dropout": None,
                                                         "cls_dense_layer_number_of_options": 1,
                                                         "gating_block_num_layers": 1,
                                                         "gating_block_end": True, "gating_block_end_position": 9,
                                                         "gating_block_middle": False,
                                                         "gating_block_middle_position": 6,
                                                         "gating_block_start": False, "gating_block_start_position": 3,
                                                         "nm_gating": False, "is_diagnostics": False,
                                                         "max_position_embeddings": 512, "max_seq_len": 512, "model_type":"bert"}
        self.bert_base_cased_no_gating_middle_only_1_layers_512sl = {"vocab_size": 28996, "hidden_size": 768,
                                                            "num_hidden_layers": 12,
                                                            "num_attention_heads": 12, "intermediate_size": 3072,
                                                            "hidden_act": "gelu", "hidden_dropout_prob": 0.1,
                                                            "attention_probs_dropout_prob": 0.1, "type_vocab_size": 2,
                                                            "initializer_range": 0.02, "layer_norm_eps": 1e-12,
                                                            "pad_token_id": 0, "position_embedding_type": "absolute",
                                                            "use_cache": True, "classifier_dropout": None,
                                                            "cls_dense_layer_number_of_options": 1,
                                                            "gating_block_num_layers": 1,
                                                            "gating_block_end": False, "gating_block_end_position": 9,
                                                            "gating_block_middle": True,
                                                            "gating_block_middle_position": 6,
                                                            "gating_block_start": False,
                                                            "gating_block_start_position": 3,
                                                            "nm_gating": False, "is_diagnostics": False,
                                                            "max_position_embeddings": 512, "max_seq_len": 512, "model_type":"bert"}
        self.bert_base_cased_no_gating_start_only_1_layers_512sl = {"vocab_size": 28996, "hidden_size": 768,
                                                           "num_hidden_layers": 12,
                                                           "num_attention_heads": 12, "intermediate_size": 3072,
                                                           "hidden_act": "gelu", "hidden_dropout_prob": 0.1,
                                                           "attention_probs_dropout_prob": 0.1, "type_vocab_size": 2,
                                                           "initializer_range": 0.02, "layer_norm_eps": 1e-12,
                                                           "pad_token_id": 0, "position_embedding_type": "absolute",
                                                           "use_cache": True, "classifier_dropout": None,
                                                           "cls_dense_layer_number_of_options": 1,
                                                           "gating_block_num_layers": 1,
                                                           "gating_block_end": False, "gating_block_end_position": 9,
                                                           "gating_block_middle": False,
                                                           "gating_block_middle_position": 6,
                                                           "gating_block_start": True, "gating_block_start_position": 3,
                                                           "nm_gating": False, "is_diagnostics": False,
                                                           "max_position_embeddings": 512, "max_seq_len": 512, "model_type":"bert"}

        #https://huggingface.co/bert-base-cased/blob/main/config.json for bert-base-cased configuration.
        self.bert_base_cased_original_512sl = {"vocab_size":28996, "hidden_size":768,"num_hidden_layers":12,
                                         "num_attention_heads":12,"intermediate_size":3072,
                                         "hidden_act":"gelu","hidden_dropout_prob":0.1,
                                         "attention_probs_dropout_prob":0.1, "type_vocab_size":2,
                                         "initializer_range":0.02,"layer_norm_eps":1e-12,
                                         "pad_token_id":0,"position_embedding_type":"absolute",
                                         "use_cache":True,"classifier_dropout":None,
                                         "cls_dense_layer_number_of_options":1,"gating_block_num_layers":0,
                                         "gating_block_end":False, "gating_block_end_position":9,
                                         "gating_block_middle":False, "gating_block_middle_position":6,
                                         "gating_block_start":False, "gating_block_start_position":3,
                                         "nm_gating":False,"is_diagnostics":False,
                                         "max_position_embeddings":512, "max_seq_len":512, "model_type":"bert"}


        #https://huggingface.co/bert-large-cased/blob/main/config.json for bert-large-cased configuration.
        self.bert_large_cased_gating_all_3_layers_512sl = {"vocab_size":28996, "hidden_size":1024,"num_hidden_layers":24,
                                                     "num_attention_heads":16,"intermediate_size":4096,
                                                     "hidden_act":"gelu","hidden_dropout_prob":0.1,
                                                     "attention_probs_dropout_prob":0.1, "type_vocab_size":2,
                                                     "initializer_range":0.02,"layer_norm_eps":1e-12,
                                                     "pad_token_id":0,"position_embedding_type":"absolute",
                                                     "use_cache":True,"classifier_dropout":None,
                                                     "cls_dense_layer_number_of_options":1,"gating_block_num_layers":3,
                                                     "gating_block_end":True, "gating_block_end_position":21,
                                                     "gating_block_middle":True, "gating_block_middle_position":12,
                                                     "gating_block_start":True, "gating_block_start_position":3,
                                                     "nm_gating":True,"is_diagnostics":False,
                                                     "max_position_embeddings":512, "max_seq_len":512, "model_type":"bert"}
        self.bert_large_cased_gating_end_only_3_layers_512sl = {"vocab_size":28996, "hidden_size":1024,"num_hidden_layers":24,
                                                     "num_attention_heads":16,"intermediate_size":4096,
                                                     "hidden_act":"gelu","hidden_dropout_prob":0.1,
                                                     "attention_probs_dropout_prob":0.1, "type_vocab_size":2,
                                                     "initializer_range":0.02,"layer_norm_eps":1e-12,
                                                     "pad_token_id":0,"position_embedding_type":"absolute",
                                                     "use_cache":True,"classifier_dropout":None,
                                                     "cls_dense_layer_number_of_options":1,"gating_block_num_layers":3,
                                                     "gating_block_end":True, "gating_block_end_position":21,
                                                     "gating_block_middle":False, "gating_block_middle_position":12,
                                                     "gating_block_start":False, "gating_block_start_position":3,
                                                     "nm_gating":True,"is_diagnostics":False,
                                                     "max_position_embeddings":512, "max_seq_len":512, "model_type":"bert"}
        self.bert_large_cased_gating_middle_only_3_layers_512sl = {"vocab_size":28996, "hidden_size":1024,"num_hidden_layers":24,
                                                     "num_attention_heads":16,"intermediate_size":4096,
                                                     "hidden_act":"gelu","hidden_dropout_prob":0.1,
                                                     "attention_probs_dropout_prob":0.1, "type_vocab_size":2,
                                                     "initializer_range":0.02,"layer_norm_eps":1e-12,
                                                     "pad_token_id":0,"position_embedding_type":"absolute",
                                                     "use_cache":True,"classifier_dropout":None,
                                                     "cls_dense_layer_number_of_options":1,"gating_block_num_layers":3,
                                                     "gating_block_end":False, "gating_block_end_position":21,
                                                     "gating_block_middle":True, "gating_block_middle_position":12,
                                                     "gating_block_start":False, "gating_block_start_position":3,
                                                     "nm_gating":True,"is_diagnostics":False,
                                                     "max_position_embeddings":512, "max_seq_len":512, "model_type":"bert"}
        self.bert_large_cased_gating_start_only_3_layers_512sl = {"vocab_size":28996, "hidden_size":1024,"num_hidden_layers":24,
                                                     "num_attention_heads":16,"intermediate_size":4096,
                                                     "hidden_act":"gelu","hidden_dropout_prob":0.1,
                                                     "attention_probs_dropout_prob":0.1, "type_vocab_size":2,
                                                     "initializer_range":0.02,"layer_norm_eps":1e-12,
                                                     "pad_token_id":0,"position_embedding_type":"absolute",
                                                     "use_cache":True,"classifier_dropout":None,
                                                     "cls_dense_layer_number_of_options":1,"gating_block_num_layers":3,
                                                     "gating_block_end":False, "gating_block_end_position":21,
                                                     "gating_block_middle":False, "gating_block_middle_position":12,
                                                     "gating_block_start":True, "gating_block_start_position":3,
                                                     "nm_gating":True,"is_diagnostics":False,
                                                     "max_position_embeddings":512, "max_seq_len":512, "model_type":"bert"}

        # https://huggingface.co/bert-large-cased/blob/main/config.json for bert-large-cased configuration.
        self.bert_large_cased_gating_all_1_layers_512sl = {"vocab_size":28996, "hidden_size":1024,"num_hidden_layers":24,
                                                     "num_attention_heads":16,"intermediate_size":4096,
                                                     "hidden_act":"gelu","hidden_dropout_prob":0.1,
                                                     "attention_probs_dropout_prob":0.1, "type_vocab_size":2,
                                                     "initializer_range":0.02,"layer_norm_eps":1e-12,
                                                     "pad_token_id":0,"position_embedding_type":"absolute",
                                                     "use_cache":True,"classifier_dropout":None,
                                                     "cls_dense_layer_number_of_options":1,"gating_block_num_layers":1,
                                                     "gating_block_end":True, "gating_block_end_position":21,
                                                     "gating_block_middle":True, "gating_block_middle_position":12,
                                                     "gating_block_start":True, "gating_block_start_position":3,
                                                     "nm_gating":True,"is_diagnostics":False,
                                                     "max_position_embeddings":512, "max_seq_len":512, "model_type":"bert"}
        self.bert_large_cased_gating_end_only_1_layers_512sl = {"vocab_size":28996, "hidden_size":1024,"num_hidden_layers":24,
                                                     "num_attention_heads":16,"intermediate_size":4096,
                                                     "hidden_act":"gelu","hidden_dropout_prob":0.1,
                                                     "attention_probs_dropout_prob":0.1, "type_vocab_size":2,
                                                     "initializer_range":0.02,"layer_norm_eps":1e-12,
                                                     "pad_token_id":0,"position_embedding_type":"absolute",
                                                     "use_cache":True,"classifier_dropout":None,
                                                     "cls_dense_layer_number_of_options":1,"gating_block_num_layers":1,
                                                     "gating_block_end":True, "gating_block_end_position":21,
                                                     "gating_block_middle":False, "gating_block_middle_position":12,
                                                     "gating_block_start":False, "gating_block_start_position":3,
                                                     "nm_gating":True,"is_diagnostics":False,
                                                     "max_position_embeddings":512, "max_seq_len":512, "model_type":"bert"}
        self.bert_large_cased_gating_middle_only_1_layers_512sl = {"vocab_size":28996, "hidden_size":1024,"num_hidden_layers":24,
                                                     "num_attention_heads":16,"intermediate_size":4096,
                                                     "hidden_act":"gelu","hidden_dropout_prob":0.1,
                                                     "attention_probs_dropout_prob":0.1, "type_vocab_size":2,
                                                     "initializer_range":0.02,"layer_norm_eps":1e-12,
                                                     "pad_token_id":0,"position_embedding_type":"absolute",
                                                     "use_cache":True,"classifier_dropout":None,
                                                     "cls_dense_layer_number_of_options":1,"gating_block_num_layers":1,
                                                     "gating_block_end":False, "gating_block_end_position":21,
                                                     "gating_block_middle":True, "gating_block_middle_position":12,
                                                     "gating_block_start":False, "gating_block_start_position":3,
                                                     "nm_gating":True,"is_diagnostics":False,
                                                     "max_position_embeddings":512, "max_seq_len":512, "model_type":"bert"}
        self.bert_large_cased_gating_start_only_1_layers_512sl = {"vocab_size":28996, "hidden_size":1024,"num_hidden_layers":24,
                                                     "num_attention_heads":16,"intermediate_size":4096,
                                                     "hidden_act":"gelu","hidden_dropout_prob":0.1,
                                                     "attention_probs_dropout_prob":0.1, "type_vocab_size":2,
                                                     "initializer_range":0.02,"layer_norm_eps":1e-12,
                                                     "pad_token_id":0,"position_embedding_type":"absolute",
                                                     "use_cache":True,"classifier_dropout":None,
                                                     "cls_dense_layer_number_of_options":1,"gating_block_num_layers":1,
                                                     "gating_block_end":False, "gating_block_end_position":21,
                                                     "gating_block_middle":False, "gating_block_middle_position":12,
                                                     "gating_block_start":True, "gating_block_start_position":3,
                                                     "nm_gating":True,"is_diagnostics":False,
                                                     "max_position_embeddings":512, "max_seq_len":512, "model_type":"bert"}

        # https://huggingface.co/bert-large-cased/blob/main/config.json for bert-large-cased configuration.
        self.bert_large_cased_no_gating_all_3_layers_512sl = {"vocab_size": 28996, "hidden_size": 1024,
                                                           "num_hidden_layers": 24,
                                                           "num_attention_heads": 16, "intermediate_size": 4096,
                                                           "hidden_act": "gelu", "hidden_dropout_prob": 0.1,
                                                           "attention_probs_dropout_prob": 0.1, "type_vocab_size": 2,
                                                           "initializer_range": 0.02, "layer_norm_eps": 1e-12,
                                                           "pad_token_id": 0, "position_embedding_type": "absolute",
                                                           "use_cache": True, "classifier_dropout": None,
                                                           "cls_dense_layer_number_of_options": 1,
                                                           "gating_block_num_layers": 3,
                                                           "gating_block_end": True, "gating_block_end_position":21,
                                                           "gating_block_middle": True,
                                                           "gating_block_middle_position": 12,
                                                           "gating_block_start": True, "gating_block_start_position": 3,
                                                           "nm_gating": False, "is_diagnostics": False,
                                                           "max_position_embeddings": 512, "max_seq_len": 512, "model_type":"bert"}
        self.bert_large_cased_no_gating_end_only_3_layers_512sl = {"vocab_size": 28996, "hidden_size": 1024,
                                                                "num_hidden_layers": 24,
                                                                "num_attention_heads": 16, "intermediate_size": 4096,
                                                                "hidden_act": "gelu", "hidden_dropout_prob": 0.1,
                                                                "attention_probs_dropout_prob": 0.1,
                                                                "type_vocab_size": 2,
                                                                "initializer_range": 0.02, "layer_norm_eps": 1e-12,
                                                                "pad_token_id": 0,
                                                                "position_embedding_type": "absolute",
                                                                "use_cache": True, "classifier_dropout": None,
                                                                "cls_dense_layer_number_of_options": 1,
                                                                "gating_block_num_layers": 3,
                                                                "gating_block_end": True,
                                                                "gating_block_end_position": 21,
                                                                "gating_block_middle": False,
                                                                "gating_block_middle_position": 12,
                                                                "gating_block_start": False,
                                                                "gating_block_start_position": 3,
                                                                "nm_gating": False, "is_diagnostics": False,
                                                                "max_position_embeddings": 512, "max_seq_len": 512, "model_type":"bert"}
        self.bert_large_cased_no_gating_middle_only_3_layers_512sl = {"vocab_size": 28996, "hidden_size": 1024,
                                                                   "num_hidden_layers": 24,
                                                                   "num_attention_heads": 16, "intermediate_size": 4096,
                                                                   "hidden_act": "gelu", "hidden_dropout_prob": 0.1,
                                                                   "attention_probs_dropout_prob": 0.1,
                                                                   "type_vocab_size": 2,
                                                                   "initializer_range": 0.02, "layer_norm_eps": 1e-12,
                                                                   "pad_token_id": 0,
                                                                   "position_embedding_type": "absolute",
                                                                   "use_cache": True, "classifier_dropout": None,
                                                                   "cls_dense_layer_number_of_options": 1,
                                                                   "gating_block_num_layers": 3,
                                                                   "gating_block_end": False,
                                                                   "gating_block_end_position": 21,
                                                                   "gating_block_middle": True,
                                                                   "gating_block_middle_position": 12,
                                                                   "gating_block_start": False,
                                                                   "gating_block_start_position": 3,
                                                                   "nm_gating": False, "is_diagnostics": False,
                                                                   "max_position_embeddings": 512, "max_seq_len": 512, "model_type":"bert"}
        self.bert_large_cased_no_gating_start_only_3_layers_512sl = {"vocab_size": 28996, "hidden_size": 1024,
                                                                  "num_hidden_layers": 24,
                                                                  "num_attention_heads": 16, "intermediate_size": 4096,
                                                                  "hidden_act": "gelu", "hidden_dropout_prob": 0.1,
                                                                  "attention_probs_dropout_prob": 0.1,
                                                                  "type_vocab_size": 2,
                                                                  "initializer_range": 0.02, "layer_norm_eps": 1e-12,
                                                                  "pad_token_id": 0,
                                                                  "position_embedding_type": "absolute",
                                                                  "use_cache": True, "classifier_dropout": None,
                                                                  "cls_dense_layer_number_of_options": 1,
                                                                  "gating_block_num_layers": 3,
                                                                  "gating_block_end": False,
                                                                  "gating_block_end_position": 21,
                                                                  "gating_block_middle": False,
                                                                  "gating_block_middle_position": 12,
                                                                  "gating_block_start": True,
                                                                  "gating_block_start_position": 3,
                                                                  "nm_gating": False, "is_diagnostics": False,
                                                                  "max_position_embeddings": 512, "max_seq_len": 512, "model_type":"bert"}

        # https://huggingface.co/bert-large-cased/blob/main/config.json for bert-large-cased configuration.
        self.bert_large_cased_no_gating_all_1_layers_512sl = {"vocab_size": 28996, "hidden_size": 1024,
                                                           "num_hidden_layers": 24,
                                                           "num_attention_heads": 16, "intermediate_size": 4096,
                                                           "hidden_act": "gelu", "hidden_dropout_prob": 0.1,
                                                           "attention_probs_dropout_prob": 0.1, "type_vocab_size": 2,
                                                           "initializer_range": 0.02, "layer_norm_eps": 1e-12,
                                                           "pad_token_id": 0, "position_embedding_type": "absolute",
                                                           "use_cache": True, "classifier_dropout": None,
                                                           "cls_dense_layer_number_of_options": 1,
                                                           "gating_block_num_layers": 1,
                                                           "gating_block_end": True, "gating_block_end_position": 21,
                                                           "gating_block_middle": True,
                                                           "gating_block_middle_position": 12,
                                                           "gating_block_start": True, "gating_block_start_position": 3,
                                                           "nm_gating": False, "is_diagnostics": False,
                                                           "max_position_embeddings": 512, "max_seq_len": 512, "model_type":"bert"}
        self.bert_large_cased_no_gating_end_only_1_layers_512sl = {"vocab_size": 28996, "hidden_size": 1024,
                                                                "num_hidden_layers": 24,
                                                                "num_attention_heads": 16, "intermediate_size": 4096,
                                                                "hidden_act": "gelu", "hidden_dropout_prob": 0.1,
                                                                "attention_probs_dropout_prob": 0.1,
                                                                "type_vocab_size": 2,
                                                                "initializer_range": 0.02, "layer_norm_eps": 1e-12,
                                                                "pad_token_id": 0,
                                                                "position_embedding_type": "absolute",
                                                                "use_cache": True, "classifier_dropout": None,
                                                                "cls_dense_layer_number_of_options": 1,
                                                                "gating_block_num_layers": 1,
                                                                "gating_block_end": True,
                                                                "gating_block_end_position": 21,
                                                                "gating_block_middle": False,
                                                                "gating_block_middle_position": 12,
                                                                "gating_block_start": False,
                                                                "gating_block_start_position": 3,
                                                                "nm_gating": False, "is_diagnostics": False,
                                                                "max_position_embeddings": 512, "max_seq_len": 512, "model_type":"bert"}
        self.bert_large_cased_no_gating_middle_only_1_layers_512sl = {"vocab_size": 28996, "hidden_size": 1024,
                                                                   "num_hidden_layers": 24,
                                                                   "num_attention_heads": 16, "intermediate_size": 4096,
                                                                   "hidden_act": "gelu", "hidden_dropout_prob": 0.1,
                                                                   "attention_probs_dropout_prob": 0.1,
                                                                   "type_vocab_size": 2,
                                                                   "initializer_range": 0.02, "layer_norm_eps": 1e-12,
                                                                   "pad_token_id": 0,
                                                                   "position_embedding_type": "absolute",
                                                                   "use_cache": True, "classifier_dropout": None,
                                                                   "cls_dense_layer_number_of_options": 1,
                                                                   "gating_block_num_layers": 1,
                                                                   "gating_block_end": False,
                                                                   "gating_block_end_position": 21,
                                                                   "gating_block_middle": True,
                                                                   "gating_block_middle_position": 12,
                                                                   "gating_block_start": False,
                                                                   "gating_block_start_position": 3,
                                                                   "nm_gating": False, "is_diagnostics": False,
                                                                   "max_position_embeddings": 512, "max_seq_len": 512, "model_type":"bert"}
        self.bert_large_cased_no_gating_start_only_1_layers_512sl = {"vocab_size": 28996, "hidden_size": 1024,
                                                                  "num_hidden_layers": 24,
                                                                  "num_attention_heads": 16, "intermediate_size": 4096,
                                                                  "hidden_act": "gelu", "hidden_dropout_prob": 0.1,
                                                                  "attention_probs_dropout_prob": 0.1,
                                                                  "type_vocab_size": 2,
                                                                  "initializer_range": 0.02, "layer_norm_eps": 1e-12,
                                                                  "pad_token_id": 0,
                                                                  "position_embedding_type": "absolute",
                                                                  "use_cache": True, "classifier_dropout": None,
                                                                  "cls_dense_layer_number_of_options": 1,
                                                                  "gating_block_num_layers": 1,
                                                                  "gating_block_end": False,
                                                                  "gating_block_end_position": 21,
                                                                  "gating_block_middle": False,
                                                                  "gating_block_middle_position": 12,
                                                                  "gating_block_start": True,
                                                                  "gating_block_start_position": 3,
                                                                  "nm_gating": False, "is_diagnostics": False,
                                                                  "max_position_embeddings": 512, "max_seq_len": 512, "model_type":"bert"}

        # https://huggingface.co/bert-large-cased/blob/main/config.json for bert-large-cased configuration.
        self.bert_large_cased_original_512sl = {"vocab_size":28996, "hidden_size":1024,"num_hidden_layers":24,
                                                "num_attention_heads":16,"intermediate_size":4096,
                                                "hidden_act":"gelu","hidden_dropout_prob":0.1,
                                                "attention_probs_dropout_prob":0.1, "type_vocab_size":2,
                                                "initializer_range":0.02,"layer_norm_eps":1e-12,
                                                "pad_token_id":0,"position_embedding_type":"absolute",
                                                "use_cache":True,"classifier_dropout":None,
                                                "cls_dense_layer_number_of_options":1,"gating_block_num_layers":0,
                                                "gating_block_end":False, "gating_block_end_position":21,
                                                "gating_block_middle":False, "gating_block_middle_position":12,
                                                "gating_block_start":False, "gating_block_start_position":3,
                                                "nm_gating":False,"is_diagnostics":False,
                                                "max_position_embeddings":512, "max_seq_len":512, "model_type":"bert"}

    def return_config_dictionary(self, type_):

        # base models here.
        if type_ == "bert_base_cased_gating_all_3_layers_512sl": return self.bert_base_cased_gating_all_3_layers_512sl
        elif type_ == "bert_base_cased_gating_end_only_3_layers_512sl": return self.bert_base_cased_gating_end_only_3_layers_512sl
        elif type_ == "bert_base_cased_gating_middle_only_3_layers_512sl": return self.bert_base_cased_gating_middle_only_3_layers_512sl
        elif type_ == "bert_base_cased_gating_start_only_3_layers_512sl": return self.bert_base_cased_gating_start_only_3_layers_512sl

        elif type_ == "bert_base_cased_gating_all_1_layers_512sl": return self.bert_base_cased_gating_all_1_layers_512sl
        elif type_ == "bert_base_cased_gating_end_only_1_layers_512sl": return self.bert_base_cased_gating_end_only_1_layers_512sl
        elif type_ == "bert_base_cased_gating_middle_only_1_layers_512sl": return self.bert_base_cased_gating_middle_only_1_layers_512sl
        elif type_ == "bert_base_cased_gating_start_only_1_layers_512sl": return self.bert_base_cased_gating_start_only_1_layers_512sl

        elif type_ == "bert_base_cased_no_gating_all_3_layers_512sl": return self.bert_base_cased_no_gating_all_3_layers_512sl
        elif type_ == "bert_base_cased_no_gating_end_only_3_layers_512sl": return self.bert_base_cased_no_gating_end_only_3_layers_512sl
        elif type_ == "bert_base_cased_no_gating_middle_only_3_layers_512sl": return self.bert_base_cased_no_gating_middle_only_3_layers_512sl
        elif type_ == "bert_base_cased_no_gating_start_only_3_layers_512sl": return self.bert_base_cased_no_gating_start_only_3_layers_512sl

        elif type_ == "bert_base_cased_no_gating_all_1_layers_512sl": return self.bert_base_cased_no_gating_all_1_layers_512sl
        elif type_ == "bert_base_cased_no_gating_end_only_1_layers_512sl": return self.bert_base_cased_no_gating_end_only_1_layers_512sl
        elif type_ == "bert_base_cased_no_gating_middle_only_1_layers_512sl": return self.bert_base_cased_no_gating_middle_only_1_layers_512sl
        elif type_ == "bert_base_cased_no_gating_start_only_1_layers_512sl": return self.bert_base_cased_no_gating_start_only_1_layers_512sl

        elif type_ == "bert_base_cased_original_512sl": return self.bert_base_cased_original_512sl

        # large models here.
        elif type_ == "bert_large_cased_gating_all_3_layers_512sl": return self.bert_large_cased_gating_all_3_layers_512sl
        elif type_ == "bert_large_cased_gating_end_only_3_layers_512sl": return self.bert_large_cased_gating_end_only_3_layers_512sl
        elif type_ == "bert_large_cased_gating_middle_only_3_layers_512sl": return self.bert_large_cased_gating_middle_only_3_layers_512sl
        elif type_ == "bert_large_cased_gating_start_only_3_layers_512sl": return self.bert_large_cased_gating_start_only_3_layers_512sl

        elif type_ == "bert_large_cased_gating_all_1_layers_512sl": return self.bert_large_cased_gating_all_1_layers_512sl
        elif type_ == "bert_large_cased_gating_end_only_1_layers_512sl": return self.bert_large_cased_gating_end_only_1_layers_512sl
        elif type_ == "bert_large_cased_gating_middle_only_1_layers_512sl": return self.bert_large_cased_gating_middle_only_1_layers_512sl
        elif type_ == "bert_large_cased_gating_start_only_1_layers_512sl": return self.bert_large_cased_gating_start_only_1_layers_512sl

        elif type_ == "bert_large_cased_no_gating_all_3_layers_512sl": return self.bert_large_cased_no_gating_all_3_layers_512sl
        elif type_ == "bert_large_cased_no_gating_end_only_3_layers_512sl": return self.bert_large_cased_no_gating_end_only_3_layers_512sl
        elif type_ == "bert_large_cased_no_gating_middle_only_3_layers_512sl": return self.bert_large_cased_no_gating_middle_only_3_layers_512sl
        elif type_ == "bert_large_cased_no_gating_start_only_3_layers_512sl": return self.bert_large_cased_no_gating_start_only_3_layers_512sl

        elif type_ == "bert_large_cased_no_gating_all_1_layers_512sl": return self.bert_large_cased_no_gating_all_1_layers_512sl
        elif type_ == "bert_large_cased_no_gating_end_only_1_layers_512sl": return self.bert_large_cased_no_gating_end_only_1_layers_512sl
        elif type_ == "bert_large_cased_no_gating_middle_only_1_layers_512sl": return self.bert_large_cased_no_gating_middle_only_1_layers_512sl
        elif type_ == "bert_large_cased_no_gating_start_only_1_layers_512sl": return self.bert_large_cased_no_gating_start_only_1_layers_512sl

        elif type_ == "bert_large_cased_original_512sl": return self.bert_large_cased_original_512sl
        else: raise Exception(f"Invalid type_: {type_}")





